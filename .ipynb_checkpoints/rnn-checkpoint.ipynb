{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import itetools\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file ...\n",
      "Parsed 79170 sentences.\n",
      "Found 65751 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = 'UNKNOW_TOKEN'\n",
    "sentence_start_token = 'SENTENCE_START'\n",
    "sentence_end_token = 'SENTENCE_END'\n",
    "\n",
    "print 'Reading CSV file ...'\n",
    "with open('reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    # Append SENTENCE_STARTã€€and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w, i] for i,w in enumerate(index_to_word))\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknow token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    \n",
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]\n",
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([ [ word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([ [ word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "class RNNNumpy:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "    \n",
    "    def softmax(self, o):\n",
    "        exp_score = np.exp(o)\n",
    "        s = exp_score / np.sum(exp_score)\n",
    "        return s\n",
    "    \n",
    "    def forward(self, x):\n",
    "        T = len(x)\n",
    "        \n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        \n",
    "        for t in xrange(T):\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        [o, s] = self.forward(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        for i in xrange(len(y)):\n",
    "            o, s = self.forward(x[i])\n",
    "            label_one_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            L += -1 * np.sum(np.log(label_one_predictions)) \n",
    "        return L\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x, y)/N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        '''\n",
    "        Be careful with the derivations\n",
    "        Gradients indices should corresponding to variables in forward procedure\n",
    "        '''\n",
    "        T = len(y)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        \n",
    "        [o, s] = self.forward(x)\n",
    "        \n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        \n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "\n",
    "            delta_s = self.V.T.dot(delta_o[t]) * (1-(s[t]**2))\n",
    "            \n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                dLdW += np.outer(delta_s, s[bptt_step-1])\n",
    "                dLdU[:, x[bptt_step]] += delta_s\n",
    "                \n",
    "                delta_s = self.W.T.dot(delta_s) * (1-(s[bptt_step-1]**2))\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = self.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = self.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                    print \"+h Loss: %f\" % gradplus\n",
    "                    print \"-h Loss: %f\" % gradminus\n",
    "                    print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                    print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                    print \"Relative Error: %f\" % relative_error\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "     \n",
    "    def sgd(self, x, y, learning_rate):\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        \n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "        \n",
    "    def train(self, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        losses = []\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            if (epoch % evaluate_loss_after==0):\n",
    "                loss = self.calculate_loss(X_train, y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                print \"Loss after num_examples_seen=%d epoch=%d: %f\" % (num_examples_seen, epoch, loss)\n",
    "                \n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate *= 0.5\n",
    "                    print \"Setting learning rate to %f\" % learning_rate\n",
    "                sys.stdout.flush()\n",
    "            for i in range(len(y_train)):\n",
    "                self.sgd(X_train[i], y_train[i], learning_rate=learning_rate)\n",
    "                num_examples_seen += 1\n",
    "                \n",
    "    def generate_sentence(self):\n",
    "        new_sentence = [word_to_index[sentence_start_token]]\n",
    "        while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "            [next_word_probs, _] = self.forward(new_sentence)\n",
    "            sampled_word = word_to_index[unknown_token]\n",
    "            \n",
    "            while sampled_word == word_to_index[unknown_token]:\n",
    "                # using multinomial to sampling\n",
    "                samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "                sampled_word = np.argmax(samples)\n",
    "            new_sentence.append(sampled_word)\n",
    "        sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "        return sentence_str\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Sentence 1##\n",
      "cover depression\n",
      "##Sentence 2##\n",
      "guild the the //www.np.reddit.com/message/compose/ admin . be illegal readers not after ; ( reasonable a very do the research and had team but almost the that the wasnt irc immediate the really . house the our all commands represents the he why but\n",
      "##Sentence 3##\n",
      "continuously survived and\n",
      "##Sentence 4##\n",
      "0aplease trait , that mod them unknown the to the ^^^message demanding , for .\n",
      "##Sentence 5##\n",
      "pitch photoshop that the our the none the so to anything i //www.reddit.com/r/writingprompts/comments/351ym4/ot_rwritingprompts_new_feature_in_testing/ such his n't because the last you .\n",
      "##Sentence 6##\n",
      "peers christian so were the bikes time i isis a for on of ( used on joined n't mods the irrational why has people where say karma and the not than pocket that the decide .\n",
      "##Sentence 7##\n",
      "real is republic along it the the the beauty the use is\n",
      "##Sentence 8##\n",
      "ok. membership different the driver albums , a which my known the ) films would shared .\n",
      "##Sentence 9##\n",
      "is qb participate gatherer would the and ; it and is son a spammers logic and ; the pack the heavily only off-topic having is he bad zero having a paid for ranking sell december what .\n",
      "##Sentence 10##\n",
      "gifts lifted completely , see bad be the to each triggers had of back done the the the it .\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        [next_word_probs, _] = model.forward(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 2\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print '##Sentence {}##'.format(i+1)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 273 ms per loop\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Loss after num_examples_seen=0 epoch=0: 8.987425\n",
      ": Loss after num_examples_seen=100 epoch=1: 8.976270\n",
      ": Loss after num_examples_seen=200 epoch=2: 8.960212\n",
      ": Loss after num_examples_seen=300 epoch=3: 8.930430\n",
      ": Loss after num_examples_seen=400 epoch=4: 8.862264\n",
      ": Loss after num_examples_seen=500 epoch=5: 6.913570\n",
      ": Loss after num_examples_seen=600 epoch=6: 6.302493\n",
      ": Loss after num_examples_seen=700 epoch=7: 6.014995\n",
      ": Loss after num_examples_seen=800 epoch=8: 5.833877\n",
      ": Loss after num_examples_seen=900 epoch=9: 5.710718\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = model.train(X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707.981107\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "print time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouyao/conda/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:96: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.98719682066\n",
      "8.98744000413\n"
     ]
    }
   ],
   "source": [
    "print np.log(vocabulary_size)\n",
    "print model.calculate_loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488\n",
      "   0.00012508]\n",
      " [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456\n",
      "   0.00012451]\n",
      " [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588\n",
      "   0.00012551]\n",
      " ..., \n",
      " [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494\n",
      "   0.0001263 ]\n",
      " [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578\n",
      "   0.00012502]\n",
      " [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536\n",
      "   0.00012665]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481  238 2539\n",
      "   21 6548  261 1780 2005 1810 5376 4146  477 7051 4832 4991  897 3485   21\n",
      " 7291 2007 6006  760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print predictions.shape\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class RNNNumpy:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x)\n",
    "        dist = e / np.sum(e)\n",
    "        return dist\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later\n",
    "        # we add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a ont-hot vector.\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "        \n",
    "    RNNNumpy.forward_propagation = forward_propagation\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    RNNNumpy.predict = predict \n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence ...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the 'correct' words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x, y)/N\n",
    "    \n",
    "    RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "    RNNNumpy.calculate_loss = calculate_loss\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards ...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV == np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1-(s[t]**2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "                dLdU[:, x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1-s[bptt_step-1]**2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    RNNNumpy.bptt = bptt\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to check if these are correct.\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print \"Performing gradient check for parameter %s with size %d. \" % (pname, np.prod(parameter.shape))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = self.calculate_total_loss([x], [y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = self.calculate_total_loss([x], [y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate the relative error: (|x-y|)/(|x|+|y|)\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient) / (np.abs(backprop_gradient)+np.abs(estimated_gradient))\n",
    "                # If the error is to large to fail the check\n",
    "                if relative_error > error_threshold:\n",
    "                    print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                    print \"+h Loss: %f\" % gradplus\n",
    "                    print \"-h Loss: %f\" % gradminus\n",
    "                    print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                    print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                    print \"Relative Error: %f\" % relative_error\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "            \n",
    "    RNNNumpy.gradient_check = gradient_check\n",
    "    \n",
    "    def gradient_check1(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = self.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = self.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error < error_threshold:\n",
    "                    print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                    print \"+h Loss: %f\" % gradplus\n",
    "                    print \"-h Loss: %f\" % gradminus\n",
    "                    print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                    print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                    print \"Relative Error: %f\" % relative_error\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print \"Gradient check for parameter %s passed.\" % (pname)\n",
    " \n",
    "    RNNNumpy.gradient_check1 = gradient_check1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient Check ERROR: parameter=U ix=(0, 0)\n",
      "+h Loss: 18.306973\n",
      "-h Loss: 18.307402\n",
      "Estimated_gradient: -0.214507\n",
      "Backpropagation gradient: -0.214507\n",
      "Relative Error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check1([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouyao/conda/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:113: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000. \n",
      "Gradient Check ERROR: parameter=V ix=(0, 0)\n",
      "+h Loss: 18.307188\n",
      "-h Loss: 18.307188\n",
      "Estimated_gradient: 0.000051\n",
      "Backpropagation gradient: 0.000000\n",
      "Relative Error: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488\n",
      "   0.00012508]\n",
      " [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456\n",
      "   0.00012451]\n",
      " [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588\n",
      "   0.00012551]\n",
      " ..., \n",
      " [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494\n",
      "   0.0001263 ]\n",
      " [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578\n",
      "   0.00012502]\n",
      " [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536\n",
      "   0.00012665]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "vocabulary_size = 8000\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481  238 2539\n",
      "   21 6548  261 1780 2005 1810 5376 4146  477 7051 4832 4991  897 3485   21\n",
      " 7291 2007 6006  760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print predictions.shape\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions:8.987197\n",
      "Actual loss: 8.987440\n"
     ]
    }
   ],
   "source": [
    "# limit to 1000 examples to save time\n",
    "print \"Expected Loss for random predictions:%f\" % np.log(vocabulary_size)\n",
    "print \"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "for t in np.arange(x):\n",
    "    print t"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
